{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n\n# to display all the columns of the dataframe in the notebook\npd.pandas.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:27.662836Z","iopub.execute_input":"2021-09-04T01:40:27.663578Z","iopub.status.idle":"2021-09-04T01:40:27.673113Z","shell.execute_reply.started":"2021-09-04T01:40:27.663479Z","shell.execute_reply":"2021-09-04T01:40:27.672069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 1. Gathering Data","metadata":{}},{"cell_type":"code","source":"#directory\nimport os\npath = \"/kaggle/input/house-prices-advanced-regression-techniques\"\nos.chdir(path)\n\n# load dataset\ndata = pd.read_csv('train.csv')\n\n# rows and columns of the data\nprint(data.shape)\n# visualise the dataset\ndata.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:27.674635Z","iopub.execute_input":"2021-09-04T01:40:27.674917Z","iopub.status.idle":"2021-09-04T01:40:27.877556Z","shell.execute_reply.started":"2021-09-04T01:40:27.674889Z","shell.execute_reply":"2021-09-04T01:40:27.876432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2. Preparing Data","metadata":{}},{"cell_type":"code","source":"# make a list of the variables that contain missing values\nvars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]\n\n# determine percentage of missing values\ndata[vars_with_na].isnull().mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:27.87983Z","iopub.execute_input":"2021-09-04T01:40:27.880255Z","iopub.status.idle":"2021-09-04T01:40:27.918895Z","shell.execute_reply.started":"2021-09-04T01:40:27.88021Z","shell.execute_reply":"2021-09-04T01:40:27.917727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[vars_with_na].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:27.921375Z","iopub.execute_input":"2021-09-04T01:40:27.921791Z","iopub.status.idle":"2021-09-04T01:40:27.952848Z","shell.execute_reply.started":"2021-09-04T01:40:27.921746Z","shell.execute_reply":"2021-09-04T01:40:27.951985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyse_na_value(df, var):\n    df = df.copy()\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    df[var] = np.where(df[var].isnull(), 1, 0)\n    # let's compare the median SalePrice in the observations where data is missing\n    # vs the observations where a value is available\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.show()\n\n\n# let's run the function on each variable with missing data\nfor var in vars_with_na:\n    analyse_na_value(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:27.953903Z","iopub.execute_input":"2021-09-04T01:40:27.9542Z","iopub.status.idle":"2021-09-04T01:40:30.380425Z","shell.execute_reply.started":"2021-09-04T01:40:27.954171Z","shell.execute_reply":"2021-09-04T01:40:30.37937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make list of numerical variables\nnum_vars = [var for var in data.columns if data[var].dtypes != 'O']\nprint('Number of numerical variables: ', len(num_vars))\n\n# visualise the numerical variables\ndata[num_vars].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:30.381696Z","iopub.execute_input":"2021-09-04T01:40:30.381981Z","iopub.status.idle":"2021-09-04T01:40:30.412185Z","shell.execute_reply.started":"2021-09-04T01:40:30.381954Z","shell.execute_reply":"2021-09-04T01:40:30.411228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of House Id labels: ', len(data.Id.unique()))\nprint('Number of Houses in the Dataset: ', len(data))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:30.414332Z","iopub.execute_input":"2021-09-04T01:40:30.414805Z","iopub.status.idle":"2021-09-04T01:40:30.420466Z","shell.execute_reply.started":"2021-09-04T01:40:30.414773Z","shell.execute_reply":"2021-09-04T01:40:30.419256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of variables that contain year information\nyear_vars = [var for var in num_vars if 'Yr' in var or 'Year' in var]\nyear_vars","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:30.422358Z","iopub.execute_input":"2021-09-04T01:40:30.422881Z","iopub.status.idle":"2021-09-04T01:40:30.43899Z","shell.execute_reply.started":"2021-09-04T01:40:30.422837Z","shell.execute_reply":"2021-09-04T01:40:30.438043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's explore the relationship between the year variables\n# and the house price in a bit of more detail:\ndef analyse_year_vars(df, var):\n    df = df.copy()\n    # capture difference between year variable and year\n    # in which the house was sold\n    df[var] = df['YrSold'] - df[var]\n    plt.scatter(df[var], df['SalePrice'])\n    plt.ylabel('SalePrice')\n    plt.xlabel(var)\n    plt.show()\n    \nfor var in year_vars:\n    if var !='YrSold':\n        analyse_year_vars(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:30.441548Z","iopub.execute_input":"2021-09-04T01:40:30.44183Z","iopub.status.idle":"2021-09-04T01:40:30.889548Z","shell.execute_reply.started":"2021-09-04T01:40:30.441802Z","shell.execute_reply":"2021-09-04T01:40:30.888817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  let's male a list of discrete variables\ndiscrete_vars = [var for var in num_vars if len(\n    data[var].unique()) < 20 and var not in year_vars+['Id']]\nprint('Number of discrete variables: ', len(discrete_vars))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:30.890753Z","iopub.execute_input":"2021-09-04T01:40:30.891319Z","iopub.status.idle":"2021-09-04T01:40:30.905783Z","shell.execute_reply.started":"2021-09-04T01:40:30.891274Z","shell.execute_reply":"2021-09-04T01:40:30.904521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualise the discrete variables\ndata[discrete_vars].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:30.907128Z","iopub.execute_input":"2021-09-04T01:40:30.907421Z","iopub.status.idle":"2021-09-04T01:40:30.922164Z","shell.execute_reply.started":"2021-09-04T01:40:30.90739Z","shell.execute_reply":"2021-09-04T01:40:30.92105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyse_discrete(df, var):\n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.ylabel('Median SalePrice')\n    plt.show()\n    \nfor var in discrete_vars:\n    analyse_discrete(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:30.92354Z","iopub.execute_input":"2021-09-04T01:40:30.923822Z","iopub.status.idle":"2021-09-04T01:40:33.019046Z","shell.execute_reply.started":"2021-09-04T01:40:30.923797Z","shell.execute_reply":"2021-09-04T01:40:33.018121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make list of continuous variables\ncont_vars = [\n    var for var in num_vars if var not in discrete_vars+year_vars+['Id']]\nprint('Number of continuous variables: ', len(cont_vars))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:33.020528Z","iopub.execute_input":"2021-09-04T01:40:33.02092Z","iopub.status.idle":"2021-09-04T01:40:33.026855Z","shell.execute_reply.started":"2021-09-04T01:40:33.020877Z","shell.execute_reply":"2021-09-04T01:40:33.025905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualise the continuous variables\ndata[cont_vars].head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:33.028085Z","iopub.execute_input":"2021-09-04T01:40:33.028364Z","iopub.status.idle":"2021-09-04T01:40:33.053351Z","shell.execute_reply.started":"2021-09-04T01:40:33.028336Z","shell.execute_reply":"2021-09-04T01:40:33.052323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's go ahead and analyse the distributions of these variables\ndef analyse_continuous(df, var):\n    df = df.copy()\n    df[var].hist(bins=30)\n    plt.ylabel('Number of houses')\n    plt.xlabel(var)\n    plt.title(var)\n    plt.show()\n\nfor var in cont_vars:\n    analyse_continuous(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:33.056418Z","iopub.execute_input":"2021-09-04T01:40:33.056723Z","iopub.status.idle":"2021-09-04T01:40:36.714271Z","shell.execute_reply.started":"2021-09-04T01:40:33.056693Z","shell.execute_reply":"2021-09-04T01:40:36.713393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's go ahead and analyse the distributions of these variables\n# after applying a logarithmic transformation\ndef analyse_transformed_continuous(df, var):\n    df = df.copy()\n    # log does not take 0 or negative values, so let's be\n    # careful and skip those variables\n    if any(data[var] <= 0):\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log(df[var])\n        df[var].hist(bins=30)\n        plt.ylabel('Number of houses')\n        plt.xlabel(var)\n        plt.title(var)\n        plt.show()\n\nfor var in cont_vars:\n    analyse_transformed_continuous(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:36.715528Z","iopub.execute_input":"2021-09-04T01:40:36.715797Z","iopub.status.idle":"2021-09-04T01:40:37.632661Z","shell.execute_reply.started":"2021-09-04T01:40:36.715771Z","shell.execute_reply":"2021-09-04T01:40:37.631808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's explore the relationship between the house price and\n# the transformed variables with more detail:\ndef transform_analyse_continuous(df, var):\n    df = df.copy()\n    # log does not take negative values, so let's be careful and skip those variables\n    if any(data[var] <= 0):\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log(df[var])\n        # log transform the target (remember it was also skewed)\n        df['SalePrice'] = np.log(df['SalePrice'])\n    \n        # plot\n        plt.scatter(df[var], df['SalePrice'])\n        plt.ylabel('SalePrice')\n        plt.xlabel(var)\n        plt.show()\n\n\nfor var in cont_vars:\n    if var != 'SalePrice':\n        transform_analyse_continuous(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:37.633897Z","iopub.execute_input":"2021-09-04T01:40:37.634182Z","iopub.status.idle":"2021-09-04T01:40:38.180044Z","shell.execute_reply.started":"2021-09-04T01:40:37.634155Z","shell.execute_reply":"2021-09-04T01:40:38.179123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's make boxplots to visualise outliers in the continuous variables\ndef find_outliers(df, var):\n    df = df.copy()\n    # log does not take negative values, so let's be\n    # careful and skip those variables\n    if any(data[var] <= 0):\n        pass\n    else:\n        df[var] = np.log(df[var])\n        df.boxplot(column=var)\n        plt.title(var)\n        plt.ylabel(var)\n        plt.show()\n\n\nfor var in cont_vars:\n    find_outliers(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:38.181168Z","iopub.execute_input":"2021-09-04T01:40:38.181469Z","iopub.status.idle":"2021-09-04T01:40:38.846485Z","shell.execute_reply.started":"2021-09-04T01:40:38.18143Z","shell.execute_reply":"2021-09-04T01:40:38.845565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# capture categorical variables in a list\ncat_vars = [var for var in data.columns if data[var].dtypes == 'O']\nprint('Number of categorical variables: ', len(cat_vars))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:38.85064Z","iopub.execute_input":"2021-09-04T01:40:38.851053Z","iopub.status.idle":"2021-09-04T01:40:38.860698Z","shell.execute_reply.started":"2021-09-04T01:40:38.850997Z","shell.execute_reply":"2021-09-04T01:40:38.859792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualise the values of the categorical variables\ndata[cat_vars].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:38.863075Z","iopub.execute_input":"2021-09-04T01:40:38.863353Z","iopub.status.idle":"2021-09-04T01:40:38.907642Z","shell.execute_reply.started":"2021-09-04T01:40:38.863324Z","shell.execute_reply":"2021-09-04T01:40:38.906476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[cat_vars].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:38.908841Z","iopub.execute_input":"2021-09-04T01:40:38.909256Z","iopub.status.idle":"2021-09-04T01:40:38.944661Z","shell.execute_reply.started":"2021-09-04T01:40:38.909225Z","shell.execute_reply":"2021-09-04T01:40:38.943718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyse_rare_labels(df, var, rare_perc):\n    df = df.copy()\n    tmp = df.groupby(var)['SalePrice'].count() / len(df)\n    return tmp[tmp < rare_perc]\n\nfor var in cat_vars:\n    print(analyse_rare_labels(data, var, 0.01))\n    print()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:38.94749Z","iopub.execute_input":"2021-09-04T01:40:38.947916Z","iopub.status.idle":"2021-09-04T01:40:39.066794Z","shell.execute_reply.started":"2021-09-04T01:40:38.947878Z","shell.execute_reply":"2021-09-04T01:40:39.065831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in cat_vars:\n    # we can re-use the function to determine median\n    # sale price, that we created for discrete variables\n    analyse_discrete(data, var)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:39.068115Z","iopub.execute_input":"2021-09-04T01:40:39.068488Z","iopub.status.idle":"2021-09-04T01:40:45.128175Z","shell.execute_reply.started":"2021-09-04T01:40:39.068446Z","shell.execute_reply":"2021-09-04T01:40:45.1272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Separating the data into train and test involves randomness, therefore, we need to set the seed.","metadata":{}},{"cell_type":"code","source":"# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(data,\n                                                    data['SalePrice'],\n                                                    test_size=0.1,\n                                                    # we are setting the seed here:\n                                                    random_state=0)  \n\ntrain_X.shape, val_X.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:45.129423Z","iopub.execute_input":"2021-09-04T01:40:45.129702Z","iopub.status.idle":"2021-09-04T01:40:46.206215Z","shell.execute_reply.started":"2021-09-04T01:40:45.129675Z","shell.execute_reply":"2021-09-04T01:40:46.205337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing values","metadata":{}},{"cell_type":"code","source":"# make a list of the categorical variables that contain missing values\n\nvars_with_na = [\n    var for var in data.columns\n    if train_X[var].isnull().sum() > 0 and train_X[var].dtypes == 'O'\n]\n\n# print percentage of missing values per variable\ntrain_X[vars_with_na].isnull().mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.207491Z","iopub.execute_input":"2021-09-04T01:40:46.20776Z","iopub.status.idle":"2021-09-04T01:40:46.243186Z","shell.execute_reply.started":"2021-09-04T01:40:46.207734Z","shell.execute_reply":"2021-09-04T01:40:46.242128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace missing values with new label: \"Missing\"\ntrain_X[vars_with_na] = train_X[vars_with_na].fillna('Missing')\nval_X[vars_with_na] = val_X[vars_with_na].fillna('Missing')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.244567Z","iopub.execute_input":"2021-09-04T01:40:46.244923Z","iopub.status.idle":"2021-09-04T01:40:46.268649Z","shell.execute_reply.started":"2021-09-04T01:40:46.244889Z","shell.execute_reply":"2021-09-04T01:40:46.267422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that we have no missing information in the engineered variables\ntrain_X[vars_with_na].isnull().sum()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:46.269852Z","iopub.execute_input":"2021-09-04T01:40:46.270143Z","iopub.status.idle":"2021-09-04T01:40:46.280519Z","shell.execute_reply.started":"2021-09-04T01:40:46.270116Z","shell.execute_reply":"2021-09-04T01:40:46.279459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that test set does not contain null values in the engineered variables\n[var for var in vars_with_na if val_X[var].isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.281908Z","iopub.execute_input":"2021-09-04T01:40:46.282365Z","iopub.status.idle":"2021-09-04T01:40:46.29736Z","shell.execute_reply.started":"2021-09-04T01:40:46.282334Z","shell.execute_reply":"2021-09-04T01:40:46.296249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numerical variables","metadata":{}},{"cell_type":"code","source":"# make a list with the numerical variables that contain missing values\nvars_with_na = [\n    var for var in data.columns\n    if train_X[var].isnull().sum() > 0 and train_X[var].dtypes != 'O'\n]\n\n# print percentage of missing values per variable\ntrain_X[vars_with_na].isnull().mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.298715Z","iopub.execute_input":"2021-09-04T01:40:46.299072Z","iopub.status.idle":"2021-09-04T01:40:46.335119Z","shell.execute_reply.started":"2021-09-04T01:40:46.299034Z","shell.execute_reply":"2021-09-04T01:40:46.334135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace engineer missing values as we described above\nfor var in vars_with_na:\n\n    # calculate the mode using the train set\n    mode_val = train_X[var].mode()[0]\n\n    # add binary missing indicator (in train and test)\n    train_X[var+'_na'] = np.where(train_X[var].isnull(), 1, 0)\n    val_X[var+'_na'] = np.where(val_X[var].isnull(), 1, 0)\n\n    # replace missing values by the mode\n    # (in train and test)\n    train_X[var] = train_X[var].fillna(mode_val)\n    val_X[var] = val_X[var].fillna(mode_val)\n\n# check that we have no more missing values in the engineered variables\ntrain_X[vars_with_na].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.336679Z","iopub.execute_input":"2021-09-04T01:40:46.337092Z","iopub.status.idle":"2021-09-04T01:40:46.360826Z","shell.execute_reply.started":"2021-09-04T01:40:46.337046Z","shell.execute_reply":"2021-09-04T01:40:46.359795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that test set does not contain null values in the engineered variables\n[var for var in vars_with_na if val_X[var].isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.362327Z","iopub.execute_input":"2021-09-04T01:40:46.362612Z","iopub.status.idle":"2021-09-04T01:40:46.370045Z","shell.execute_reply.started":"2021-09-04T01:40:46.362585Z","shell.execute_reply":"2021-09-04T01:40:46.368899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the binary missing indicator variables\ntrain_X[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:46.3714Z","iopub.execute_input":"2021-09-04T01:40:46.371757Z","iopub.status.idle":"2021-09-04T01:40:46.386206Z","shell.execute_reply.started":"2021-09-04T01:40:46.371722Z","shell.execute_reply":"2021-09-04T01:40:46.385078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Temporal variables","metadata":{}},{"cell_type":"markdown","source":"Capture elapsed time: there are 4 variables that refer to the years in which the house or the garage were built or remodeled. I captured the time elapsed between those variables and the year in which the house was sold.","metadata":{}},{"cell_type":"code","source":"def elapsed_years(df, var):\n    # capture difference between the year variable\n    # and the year in which the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.387893Z","iopub.execute_input":"2021-09-04T01:40:46.388502Z","iopub.status.idle":"2021-09-04T01:40:46.393643Z","shell.execute_reply.started":"2021-09-04T01:40:46.388446Z","shell.execute_reply":"2021-09-04T01:40:46.392671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    train_X = elapsed_years(train_X, var)\n    val_X = elapsed_years(val_X, var)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.395522Z","iopub.execute_input":"2021-09-04T01:40:46.396077Z","iopub.status.idle":"2021-09-04T01:40:46.41053Z","shell.execute_reply.started":"2021-09-04T01:40:46.39603Z","shell.execute_reply":"2021-09-04T01:40:46.409513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numerical variable transformation","metadata":{}},{"cell_type":"markdown","source":"I used log transformation on the positive numerical variables in order to get a more Gaussian-like distribution. This tends to help linear machine learning models.","metadata":{}},{"cell_type":"code","source":"for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']:\n    train_X[var] = np.log(train_X[var])\n    val_X[var] = np.log(val_X[var])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.412156Z","iopub.execute_input":"2021-09-04T01:40:46.412544Z","iopub.status.idle":"2021-09-04T01:40:46.424559Z","shell.execute_reply.started":"2021-09-04T01:40:46.4125Z","shell.execute_reply":"2021-09-04T01:40:46.42372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that test set does not contain null values in the engineered variables\n[var for var in ['LotFrontage', 'LotArea', '1stFlrSF',\n                 'GrLivArea', 'SalePrice'] if val_X[var].isnull().sum() > 0]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:46.425461Z","iopub.execute_input":"2021-09-04T01:40:46.425833Z","iopub.status.idle":"2021-09-04T01:40:46.436033Z","shell.execute_reply.started":"2021-09-04T01:40:46.425792Z","shell.execute_reply":"2021-09-04T01:40:46.434874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same for train set\n[var for var in ['LotFrontage', 'LotArea', '1stFlrSF',\n                 'GrLivArea', 'SalePrice'] if train_X[var].isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.439429Z","iopub.execute_input":"2021-09-04T01:40:46.440464Z","iopub.status.idle":"2021-09-04T01:40:46.449612Z","shell.execute_reply.started":"2021-09-04T01:40:46.440426Z","shell.execute_reply":"2021-09-04T01:40:46.448424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Categorical variables","metadata":{}},{"cell_type":"markdown","source":"##### Removing rare labels\n\nFirst, I group those categories within variables that are present in less than 1% of the observations. That is, all values of categorical variables that are shared by less than 1% of houses, well be replaced by the string \"Rare\".","metadata":{}},{"cell_type":"code","source":"# let's capture the categorical variables in a list\ncat_vars = [var for var in train_X.columns if train_X[var].dtype == 'O']","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.450941Z","iopub.execute_input":"2021-09-04T01:40:46.451351Z","iopub.status.idle":"2021-09-04T01:40:46.464415Z","shell.execute_reply.started":"2021-09-04T01:40:46.451317Z","shell.execute_reply":"2021-09-04T01:40:46.46343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_frequent_labels(df, var, rare_perc):\n    \n    # function finds the labels that are shared by more than\n    # a certain % of the houses in the dataset\n    df = df.copy()\n    tmp = df.groupby(var)['SalePrice'].count() / len(df)\n    return tmp[tmp > rare_perc].index\n\n\nfor var in cat_vars:\n    \n    # find the frequent categories\n    frequent_ls = find_frequent_labels(train_X, var, 0.01)\n    \n    # replace rare categories by the string \"Rare\"\n    train_X[var] = np.where(train_X[var].isin(\n        frequent_ls), train_X[var], 'Rare')\n    \n    val_X[var] = np.where(val_X[var].isin(\n        frequent_ls), val_X[var], 'Rare')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.465788Z","iopub.execute_input":"2021-09-04T01:40:46.466294Z","iopub.status.idle":"2021-09-04T01:40:46.611645Z","shell.execute_reply.started":"2021-09-04T01:40:46.466261Z","shell.execute_reply":"2021-09-04T01:40:46.610651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Encoding of categorical variables\n\nNext, I need to transform the strings of the categorical variables into numbers. I do this to capture the monotonic relationship between the label and the target.","metadata":{}},{"cell_type":"markdown","source":"This function will assign discrete values to the strings of the variables, so that the smaller value corresponds to the category that shows the smaller mean house sale price.","metadata":{}},{"cell_type":"code","source":"def replace_categories(train, test, var, target):\n\n    # order the categories in a variable from that with the lowest\n    # house sale price, to that with the highest\n    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n\n    # create a dictionary of ordered categories to integer values\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n\n    # use the dictionary to replace the categorical strings by integers\n    train[var] = train[var].map(ordinal_label)\n    test[var] = test[var].map(ordinal_label)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.612941Z","iopub.execute_input":"2021-09-04T01:40:46.613215Z","iopub.status.idle":"2021-09-04T01:40:46.618702Z","shell.execute_reply.started":"2021-09-04T01:40:46.613188Z","shell.execute_reply":"2021-09-04T01:40:46.617629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in cat_vars:\n    replace_categories(train_X, val_X, var, 'SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.620145Z","iopub.execute_input":"2021-09-04T01:40:46.620485Z","iopub.status.idle":"2021-09-04T01:40:46.763922Z","shell.execute_reply.started":"2021-09-04T01:40:46.620454Z","shell.execute_reply":"2021-09-04T01:40:46.763074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check absence of na in the train set\n[var for var in train_X.columns if train_X[var].isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.765141Z","iopub.execute_input":"2021-09-04T01:40:46.765401Z","iopub.status.idle":"2021-09-04T01:40:46.791163Z","shell.execute_reply.started":"2021-09-04T01:40:46.765376Z","shell.execute_reply":"2021-09-04T01:40:46.790347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check absence of na in the test set\n[var for var in val_X.columns if val_X[var].isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.792313Z","iopub.execute_input":"2021-09-04T01:40:46.792572Z","iopub.status.idle":"2021-09-04T01:40:46.816446Z","shell.execute_reply.started":"2021-09-04T01:40:46.792548Z","shell.execute_reply":"2021-09-04T01:40:46.815305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let me show you what I mean by monotonic relationship\n# between labels and target\n\ndef analyse_vars(df, var):\n    \n    # function plots median house sale price per encoded\n    # category\n    \n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.ylabel('SalePrice')\n    plt.show()\n    \nfor var in cat_vars:\n    analyse_vars(train_X, var)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:46.818028Z","iopub.execute_input":"2021-09-04T01:40:46.818314Z","iopub.status.idle":"2021-09-04T01:40:52.296242Z","shell.execute_reply.started":"2021-09-04T01:40:46.818277Z","shell.execute_reply":"2021-09-04T01:40:52.295128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Scaling","metadata":{}},{"cell_type":"code","source":"# capture all variables in a list\n# except the target and the ID\n\ntrain_vars = [var for var in train_X.columns if var not in ['Id', 'SalePrice']]\n\n# count number of variables\nlen(train_vars)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:52.297616Z","iopub.execute_input":"2021-09-04T01:40:52.297903Z","iopub.status.idle":"2021-09-04T01:40:52.303985Z","shell.execute_reply.started":"2021-09-04T01:40:52.297876Z","shell.execute_reply":"2021-09-04T01:40:52.303112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# create scaler\nscaler = MinMaxScaler()\n\n#  fit  the scaler to the train set\nscaler.fit(train_X[train_vars]) \n\n# transform the train and test set\ntrain_X[train_vars] = scaler.transform(train_X[train_vars])\n\nval_X[train_vars] = scaler.transform(val_X[train_vars])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:52.30559Z","iopub.execute_input":"2021-09-04T01:40:52.306043Z","iopub.status.idle":"2021-09-04T01:40:52.405491Z","shell.execute_reply.started":"2021-09-04T01:40:52.305962Z","shell.execute_reply":"2021-09-04T01:40:52.404316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:40:52.406926Z","iopub.execute_input":"2021-09-04T01:40:52.407362Z","iopub.status.idle":"2021-09-04T01:40:52.63316Z","shell.execute_reply.started":"2021-09-04T01:40:52.407317Z","shell.execute_reply":"2021-09-04T01:40:52.632248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Selection","metadata":{}},{"cell_type":"markdown","source":"I did the model fitting and feature selection altogether in a few lines of code. First, I specified the Lasso Regression model, and we select a suitable alpha (equivalent of penalty). The bigger the alpha the less features that will be selected.\n\nThen I used the selectFromModel object from sklearn, which will select automatically the features which coefficients are non-zero.","metadata":{}},{"cell_type":"code","source":"# to build the models\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\n# remember to set the seed, the random state in this function\nsel_ = SelectFromModel(Lasso(alpha=0.005, random_state=0))\n\n# train Lasso model and select features\nsel_.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:52.634277Z","iopub.execute_input":"2021-09-04T01:40:52.63455Z","iopub.status.idle":"2021-09-04T01:40:53.018469Z","shell.execute_reply.started":"2021-09-04T01:40:52.634514Z","shell.execute_reply":"2021-09-04T01:40:53.017598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualise those features that were selected (where selected features are True).","metadata":{}},{"cell_type":"code","source":"sel_.get_support()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.019815Z","iopub.execute_input":"2021-09-04T01:40:53.020105Z","iopub.status.idle":"2021-09-04T01:40:53.026069Z","shell.execute_reply.started":"2021-09-04T01:40:53.020077Z","shell.execute_reply":"2021-09-04T01:40:53.024968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's print the number of total and selected features; this is how we can make a list of the selected features.","metadata":{}},{"cell_type":"code","source":"selected_feats = train_X.columns[(sel_.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((train_X.shape[1])))\nprint('selected features: {}'.format(len(selected_feats)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(sel_.estimator_.coef_ == 0)))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.027607Z","iopub.execute_input":"2021-09-04T01:40:53.028002Z","iopub.status.idle":"2021-09-04T01:40:53.039117Z","shell.execute_reply.started":"2021-09-04T01:40:53.02796Z","shell.execute_reply":"2021-09-04T01:40:53.038054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the selected features\nselected_feats","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.040232Z","iopub.execute_input":"2021-09-04T01:40:53.040616Z","iopub.status.idle":"2021-09-04T01:40:53.053405Z","shell.execute_reply.started":"2021-09-04T01:40:53.040584Z","shell.execute_reply":"2021-09-04T01:40:53.052481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3. Score a model","metadata":{}},{"cell_type":"code","source":"# load the train and test set with the engineered variables\nX_full = pd.read_csv('train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.056492Z","iopub.execute_input":"2021-09-04T01:40:53.056789Z","iopub.status.idle":"2021-09-04T01:40:53.087711Z","shell.execute_reply.started":"2021-09-04T01:40:53.056761Z","shell.execute_reply":"2021-09-04T01:40:53.086882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.088851Z","iopub.execute_input":"2021-09-04T01:40:53.089288Z","iopub.status.idle":"2021-09-04T01:40:53.192744Z","shell.execute_reply.started":"2021-09-04T01:40:53.089249Z","shell.execute_reply":"2021-09-04T01:40:53.19179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest Regressions","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Define the models\nmodel_1 = RandomForestRegressor(n_estimators=50, random_state=0)\nmodel_2 = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\nmodel_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\nmodel_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n\nmodels = [model_1, model_2, model_3, model_4, model_5]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.193946Z","iopub.execute_input":"2021-09-04T01:40:53.194244Z","iopub.status.idle":"2021-09-04T01:40:53.245951Z","shell.execute_reply.started":"2021-09-04T01:40:53.194214Z","shell.execute_reply":"2021-09-04T01:40:53.245031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Function for comparing different models\ndef score_model(model, X_t=train_X, X_v=val_X, y_t=train_y, y_v=val_y):\n    model.fit(X_t, y_t)\n    preds = model.predict(X_v)\n    return mean_absolute_error(y_v, preds)\n\nfor i in range(0, len(models)):\n    mae = score_model(models[i])\n    print(\"Model %d MAE: %d\" % (i+1, mae))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:40:53.250143Z","iopub.execute_input":"2021-09-04T01:40:53.250445Z","iopub.status.idle":"2021-09-04T01:41:19.865517Z","shell.execute_reply.started":"2021-09-04T01:40:53.250406Z","shell.execute_reply":"2021-09-04T01:41:19.86446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Mean Absolute Error \n\nThe goal is to see the difference in mean absolute errors (mae) each using different arguments and parameters. In Model 1, we observe the lowest n-estimate at 50 and the highest n-estimate at 200 for model 4. The only criterion equal to mae result in the lowest score, which means this calculates how good the train_test_split is. Whether we use the minimum split or max depth in both model 4 and 5, it will not result in any improvement in model performance.   ","metadata":{}},{"cell_type":"markdown","source":"#### Scoring Dataset","metadata":{}},{"cell_type":"code","source":"# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll use only numerical predictors\nX = X_full.select_dtypes(exclude=['object'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:41:19.86705Z","iopub.execute_input":"2021-09-04T01:41:19.867343Z","iopub.status.idle":"2021-09-04T01:41:19.880721Z","shell.execute_reply.started":"2021-09-04T01:41:19.867313Z","shell.execute_reply":"2021-09-04T01:41:19.8796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of training data (num_rows, num_columns)\nprint(train_X.shape)\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (train_X.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:19.882499Z","iopub.execute_input":"2021-09-04T01:41:19.88311Z","iopub.status.idle":"2021-09-04T01:41:19.893359Z","shell.execute_reply.started":"2021-09-04T01:41:19.883075Z","shell.execute_reply":"2021-09-04T01:41:19.892188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We removed rows from the missing target Sale Price in the train_X set with all the engineered features. The dependent variable will serve as Sale Price in the train_y set and this meant we dropped the Sale Price from the train_X set.\n\nUsing only numerical predictors meant that we did not have to worry about the integers and float characters. We can see that the number of missing values is great where 1095 rows and 7 columns have missing values we just omitted. This will help the performance of our model. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(train_X, train_y)\n    preds = model.predict(val_X)\n    return mean_absolute_error(val_y, preds)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:19.895102Z","iopub.execute_input":"2021-09-04T01:41:19.895497Z","iopub.status.idle":"2021-09-04T01:41:19.900656Z","shell.execute_reply.started":"2021-09-04T01:41:19.895452Z","shell.execute_reply":"2021-09-04T01:41:19.899854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Score average MAE in the training data. \nscore_dataset(train_X, val_X, train_y, val_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:19.901726Z","iopub.execute_input":"2021-09-04T01:41:19.901986Z","iopub.status.idle":"2021-09-04T01:41:21.788183Z","shell.execute_reply.started":"2021-09-04T01:41:19.901961Z","shell.execute_reply":"2021-09-04T01:41:21.787218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average MAE, using the best random forest model, returns a value close to model 2 as it is the best fit. ","metadata":{}},{"cell_type":"markdown","source":"### Step 4. Build a Model","metadata":{}},{"cell_type":"markdown","source":"### Evaluate the model","metadata":{}},{"cell_type":"code","source":"train_X.isna().sum()\ntrain_y.isna().sum()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:41:21.789662Z","iopub.execute_input":"2021-09-04T01:41:21.790128Z","iopub.status.idle":"2021-09-04T01:41:21.800004Z","shell.execute_reply.started":"2021-09-04T01:41:21.790084Z","shell.execute_reply":"2021-09-04T01:41:21.798788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Regularised linear regression: Lasso","metadata":{}},{"cell_type":"code","source":"# to build the model\nfrom sklearn.linear_model import Lasso, LassoCV\n\n# set up the model\n# remember to set the random_state / seed\nlin_model = Lasso(alpha=0.005, random_state=0)\n\n# train the model\nlin_model.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:21.801177Z","iopub.execute_input":"2021-09-04T01:41:21.801462Z","iopub.status.idle":"2021-09-04T01:41:21.861772Z","shell.execute_reply.started":"2021-09-04T01:41:21.801436Z","shell.execute_reply":"2021-09-04T01:41:21.860726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions for train set\npred = lin_model.predict(train_X)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:21.86331Z","iopub.execute_input":"2021-09-04T01:41:21.863728Z","iopub.status.idle":"2021-09-04T01:41:21.876126Z","shell.execute_reply.started":"2021-09-04T01:41:21.863684Z","shell.execute_reply":"2021-09-04T01:41:21.874676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Remember that we log transformed the output (SalePrice) in our feature engineering step. In order to get the true performance of the Lasso, I need to transform both the target and the predictions back to the original house prices values. \n\nHere, I evaluate performance using the mean squared error and the root of the mean squared error.","metadata":{}},{"cell_type":"code","source":"# to evaluate the model\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# determine mse and rmse\nprint('train mse: {}'.format(int(\n    mean_squared_error(train_y, lin_model.predict(train_X)))))\nprint('train rmse: {}'.format(int(\n    sqrt(mean_squared_error(val_y, lin_model.predict(val_X))))))\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:21.881499Z","iopub.execute_input":"2021-09-04T01:41:21.882139Z","iopub.status.idle":"2021-09-04T01:41:21.905048Z","shell.execute_reply.started":"2021-09-04T01:41:21.882083Z","shell.execute_reply":"2021-09-04T01:41:21.90279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's evaluate our predictions respect to the real sale price\nplt.scatter(val_y, lin_model.predict(val_X))\nplt.xlabel('True House Price')\nplt.ylabel('Predicted House Price')\nplt.title('Evaluation of Lasso Predictions')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:21.906692Z","iopub.execute_input":"2021-09-04T01:41:21.907162Z","iopub.status.idle":"2021-09-04T01:41:22.130696Z","shell.execute_reply.started":"2021-09-04T01:41:21.907115Z","shell.execute_reply":"2021-09-04T01:41:22.129956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our model is doing a pretty good job at estimating house prices.","metadata":{}},{"cell_type":"markdown","source":"Let's evaluate the distribution of the errors: they should be fairly normally distributed.","metadata":{}},{"cell_type":"code","source":"errors = val_y - lin_model.predict(val_X)\nerrors.hist(bins=50)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T01:41:22.131622Z","iopub.execute_input":"2021-09-04T01:41:22.131971Z","iopub.status.idle":"2021-09-04T01:41:22.419781Z","shell.execute_reply.started":"2021-09-04T01:41:22.131944Z","shell.execute_reply":"2021-09-04T01:41:22.419073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the errors follows closely a gaussian distribution. Again, that suggests that our model is doing a good job. ","metadata":{}},{"cell_type":"code","source":"# Let's look at the feature importance\nimportance = pd.Series(np.abs(lin_model.coef_.ravel()))\nimportance.index = selected_feats\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(18,6))\nplt.ylabel('Lasso Coefficients')\nplt.title('Feature Importance')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-04T01:41:22.420797Z","iopub.execute_input":"2021-09-04T01:41:22.421171Z","iopub.status.idle":"2021-09-04T01:41:23.88823Z","shell.execute_reply.started":"2021-09-04T01:41:22.421143Z","shell.execute_reply":"2021-09-04T01:41:23.887213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}